{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from function_library import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the GloVe word vectors\n",
    "url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "output = \"glove.6B.zip\"\n",
    "def download_progress(block_num, block_size, total_size):\n",
    "    progress = block_num * block_size / total_size * 100\n",
    "    print(f\"\\rDownloading: {progress:.2f}%\", end='')\n",
    "\n",
    "urllib.request.urlretrieve(url, output, reporthook=download_progress)\n",
    "\n",
    "# Use the custom ZipFileWithProgress class\n",
    "with ZipFileWithProgress(output, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"glove.6B\")\n",
    "\n",
    "# # List the extracted files\n",
    "print(os.listdir(\"glove.6B\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load glove vectors from file\n",
    "glove_file = \"glove.6B/glove.6B.300d.txt\"  \n",
    "glove_vectors = load_glove_vectors(glove_file)\n",
    "\n",
    "# Get vocabulary from glove and create embeddings\n",
    "vocab, inverse_vocab = get_vocabulary_from_glove(glove_vectors)\n",
    "embedding_dim = 300 \n",
    "vocab_size = len(glove_vectors) + 2\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "for idx, word in enumerate(inverse_vocab[2:]):\n",
    "    i = idx + 2\n",
    "    embedding.weight[i].data = glove_vectors[word]\n",
    "\n",
    "print(\"Embedding layer created with shape:\", embedding.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store dataframe in variable and clean it\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "df = df.dropna()\n",
    "df = df[df['Score'] != 3]\n",
    "df[\"Score\"] = df[\"Score\"].replace(1, 0)\n",
    "df[\"Score\"] = df[\"Score\"].replace(2, 0)\n",
    "df[\"Score\"] = df[\"Score\"].replace(5, 1)\n",
    "df[\"Score\"] = df[\"Score\"].replace(4, 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataframe in 2: Test and Train datasets\n",
    "# X represents the words, y represents the Scores\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Score'], test_size=0.2, random_state=33)\n",
    "classes = list(set(y_train))\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the Scores into binary\n",
    "y_train_bin = torch.tensor([[classes.index(y) for y in y_train]]).T\n",
    "y_test_bin = torch.tensor([[classes.index(z) for z in y_test]]).T\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer = MyTokenizer(sentence_length=100)\n",
    "tokenizer.fit(X_train)\n",
    "\n",
    "# Create training batches\n",
    "dataset = TextDataset(list(X_train), y_train_bin, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=568000, shuffle=True)\n",
    "\n",
    "# Define the model variables\n",
    "model = MyOtherClassifier(vocab_size=tokenizer.vocab_size,\n",
    "                          embedding_layer=embedding,\n",
    "                          embedding_dim=300, \n",
    "                          hidden_dim=500,\n",
    "                          output_dim=1,\n",
    "                          n_special_tokens=2,\n",
    "                          n_layers=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) # lr is the learning rate - this is our alpha\n",
    "loss_fn = nn.BCEWithLogitsLoss() # Binary Cross Entropy from Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for the necessary amount of epochs it takes to stabilize\n",
    "losses = []\n",
    "for epoch in tqdm(range(500)):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        X_train_vect, y_train_vect = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_vect)\n",
    "        loss = loss_fn(output, y_train_vect.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    losses.append(epoch_loss / len(dataloader))\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), \"model.pt\")\n",
    "torch.save(X_train_vect, \"vector.pt\")\n",
    "\n",
    "plt.figure(figsize=(3,2))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.savefig('graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = MyOtherClassifier(vocab_size=tokenizer.vocab_size,\n",
    "                          embedding_layer=embedding,\n",
    "                          embedding_dim=300, \n",
    "                          hidden_dim=500,\n",
    "                          output_dim=1,\n",
    "                          n_special_tokens=2,\n",
    "                          n_layers=3)\n",
    "\n",
    "saved_model.load_state_dict(torch.load(\"model.pt\", weights_only=True))\n",
    "saved_model.eval()\n",
    "\n",
    "my_vector = torch.load(\"vector.pt\")\n",
    "\n",
    "compilation_output = saved_model(my_vector)\n",
    "\n",
    "classification = compilation_output > 0.5\n",
    "torch.save(classification, \"classification.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classification = torch.load(\"classification.pt\")\n",
    "lista_classification = my_classification.tolist()\n",
    "print(lista_classification[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_y_train = y_train.to_list()\n",
    "sentimento_accuracy_total = 0\n",
    "sentimento_counter = 0\n",
    "for i in range(len(lista_y_train)):\n",
    "    if lista_classification[i] == [True]:\n",
    "        lista_classification[i] = 1\n",
    "    elif lista_classification[i] == [False]:\n",
    "        lista_classification[i] = 0\n",
    "        \n",
    "    if lista_y_train[i] == lista_classification[i]:\n",
    "        sentimento_accuracy_total += 1\n",
    "    sentimento_counter += 1 \n",
    "print(sentimento_accuracy_total/sentimento_counter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
